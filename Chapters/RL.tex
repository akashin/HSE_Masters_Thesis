\section{Reinforcement Learning}

Reinforcement learning usually solves sequential decision making problems. An RL agent interacts with an environment over time. At each time step $t$, the agent receives a state $s_t$ and selects an action $a_t$ from some action space $\mathcal{A}$, following a policy $\pi(a_t|s_t)$, which is the agent's behavior, i.e., a mapping from state $s_t$ to actions $a_t$, receives a scalar reward $r_t$, and transitions to the next state $s_{t+1}$, according to the environment dynamics, or model, for reward function $R(s,a)$ and state transition probability $P(s_{t+1}|s_t, a_t)$ respectively. In an episodic problem, this process continues until the agent reaches a terminal state and then it restarts. The return $R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}$ is the discounted, accumulated reward with the discount factor $\gamma \in (0,1]$. The agent aims to maximize the expectation of such long term return from each state.     

A value function is a prediction of the expected, accumulative, discounted, future reward, measuring how good is each state, or state-action pair. The action value $Q^{\pi}(s, a) = E[R_t | s_t = s, a_t = a]$ is the expected return for selecting action $a$ in state $s$ and then following policy $\pi$. An optimal action value function $Q^{*}(s, a)$ is the maximum action value achievable by any policy for state $s$ and action $a$. We can define state value $V^{\pi}(s)$ and optimal state value $V^{*}(s)$ similarly.

Temporal difference (TD) learning is a central idea in RL. It learns value function $V(s)$ directly from experience with TD error, with bootstrapping, in a model-free, online, and fully incremental way.  The update rule is $V(s_t) \leftarrow V(s_t) + \alpha [r_t + \gamma V(s_{t+1}) - V(s_t)]$, where $\alpha$ is a learning rate, and $r_t + \gamma V(s_{t+1}) - V(s_t)$ is called TD error. Similarly, Q-learning learns action value function, with the update rule, $Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r + \gamma \max_{a_{t+1}}Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t)]$. Q-learning is an off-policy control method. In contrast, SARSA, representing state, action, reward, (next) state, (next) action, is an on-policy control method, with the update rule, $Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t)]$. SARSA refines the policy greedily with respect to action values. TD-learning, Q-learning and SARSA converge under certain conditions. From optimal action value function, we can derive an optimal policy. 

The above algorithms are referred to as TD(0) and Q(0), with one-step return. We have multi-step return variants or Monte-Carlo approach in the forward view. The eligibility trace from the backward view provides an online, incremental implementation, resulting in TD($\lambda$) and Q($\lambda$) algorithms, where $\lambda \in[0,1]$. When $\lambda = 1$, it is the same as a Monte Carlo approach. 

We discuss the tabular cases above, where a value function or a policy is stored in a tabular form. Function approximation is a way for generalization when the state and/or action spaces are large or continuous. Linear function approximation used to be a popular choice, esp. before the work of Deep Q-Network~\citep{Atari}.

%For TD($\lambda$), the update rule follows, $\delta_t \leftarrow r_t + \gamma V(s_{t+1}; \theta) - \gamma V(s_t; \theta), e_t \leftarrow \gamma \lambda e_{t-1} \nabla_{\theta} + V(s_t; \theta), \theta_t \leftarrow \theta_{t-1} + \alpha \lambda e_t$.

In contrast to value-based methods like TD learning and Q-learning, policy-based methods optimize the policy $\pi(a|s; \theta)$ (with function approximation) directly, and update the parameters $\theta$ by gradient ascent on $E[R_t]$. REINFORCE is a policy gradient method, updating $\theta$ in the direction of $\nabla_{\theta} \log \pi(a_t|s_t; \theta) R_t$. Usually a baseline $b_t(s_t)$ is subtracted from the return to reduce the variance of gradient estimate, yet keeping its unbiasedness, to yield the gradient direction $\nabla_{\theta} \log \pi(a_t|s_t; \theta) (R_t - b_t(s_t))$. Using $V(s_t)$ as the baseline $b_t(s_t)$, we have the advantage function $A(a_t, s_t) = Q(a_t, s_t) - V(s_t)$, since $R_t$ is an estimate of $Q(a_t, s_t)$. In actor-critic algorithms, the critic updates action-value function parameters, and the actor updates policy parameters, in the direction suggested by the critic.

We obtain deep reinforcement learning (deep RL) methods when we use deep neural networks to approximate any of the following component of reinforcement learning: value function, $V(s; \theta)$ or $Q(s,a; \theta)$, policy $\pi(a|s; \theta)$, and model (state transition and reward). Here, the parameters $\theta$ are the weights in deep neural networks. When we use "shallow" models, like linear function, decision trees, tile coding and so on as the function approximator, we obtain "shallow" RL, and the parameters $\theta$ are the weight parameters in these models. Note, a shallow model, e.g., decision trees, may be non-linear. The distinct difference between deep RL and "shallow" RL is what function approximator is used. This is similar to the difference between deep learning and "shallow" learning. We usually utilize stochastic gradient descent to update weight parameters in deep RL. When off-policy, function approximation, in particular, non-linear function approximation, and bootstrapping are combined together, instability and divergence may occur~\citep{TDWithApproximator}. However, recent work like Deep Q-Network~\citep{Atari} and AlphaGo~\citep{AlphaGo} stabilized the learning and achieved outstanding results.

We explain some terms in RL parlance. The prediction problem, or policy evaluation, is to compute the state or action value function for a policy. The control problem is to find the optimal policy. Planning constructs a value function or a policy with a model. On-policy methods evaluate or improve the behavioural policy, e.g., SARSA fits the action-value function to the current policy, i.e., SARSA evaluates the policy based on samples from the same policy, then refines the policy greedily with respect to action values. In off-policy methods, an agent learns an optimal value function/policy, maybe following an unrelated behavioural policy, e.g., Q-learning attempts to find action values for the optimal policy directly, not necessarily fitting to the policy generating the data, i.e., the policy Q-learning obtains is usually different from the policy that generates the samples.  The notion of on-policy and off-policy can be understood as same-policy and different-policy.The exploration-exploitation dilemma is about the agent needs to exploit the currently best action to obtain rewards, yet it has to explore the environment to find better actions.  In model-free methods, the agent learns with trail-and-error from experience explicitly; the model (state transition function) is not known or learned from experience. RL methods that use models are model-based methods. In online mode, training algorithms are executed on data acquired in sequence. In batch mode, models are trained on the entire data set. With bootstrapping, an estimate of state or action value is updated from subsequent estimates.
