\begin{abstract}

\Large{
Reinforecement learning is the promising machine learning area that aims to solve hard
real-world control tasks, through automatic learning from experience and rewards.
This promise, though, comes with the unique challenges from the algorithmic and
optimization perspective, often very different from the ones faced by supervised learning
techniques. Moreover, the training procedure now involves multiple components such as
environments, agents and learners. Choosing the efficient architecture for training this
components in tandem while ensuring the best software and hardware performance can
be difficult. In this work we develop one such architecture that is able to leverage
hybrid CPU/GPU systems to train a reinforcement learning agent based on Q-learning.
We compare our implementation with DQN on benchmark tasks from Atari and VizDoom simulator
and show that it achieves faster training speed and better hardware utilization.
}

\end{abstract}

\renewcommand{\abstractname}{Аннотация}

\begin{abstract}

\Large{
Обучение с подкреплением --- многообещающая область машинного обучения, нацеленная
на решение задач оптимального управления, основання на автоматическом обучении алгоритмов
по наблюдениям и наградам. Тем не менее, задача оптимизации возникающая в данном подходе
зачастую оказывается сложнее стандартных задач обучения с учителем. Более того, в процессе
обучения принимают участие несколько разнородных компонент: окружения, агенты и учителя.
Построение эффективной архитектуры для обеспечения взаимодействия этих компонент оказывается
сложной задачей. В данной работе мы предлагаем и исследуем одну из таких архитектур, способную
использовать CPU и GPU ресурсы для обучения агента, основанного на технике Q-learning.
Мы сравниваем нашу реализацию с алгоритмом DQN на стандартных задачах из симуляторов видео-игр
Atari и VizDoom, и показываем, что она обладает более высокой скоростью обучения и более
эффективно использует ресурсы GPU.
}

\end{abstract}
