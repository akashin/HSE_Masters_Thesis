\section{Introduction}

Video games are a modern phenomena of human-computer interaction with entertainment purpose.
In a typical game, player performs an actions in virtual world that is presented through the visual interface, for example electronic display.
The user enjoyment from the game is the key metric for game developers and it depends on many factors such as user-friendly interface, good story and visuals, and the right balance of game difficulty \cite{VideoGameEnjoyment}.

Video games industry is huge, with more then 63\% American households playing video games \cite{VideoGameFacts}.
Games became more accessible with mobile and web-technologies (CITE).

% TODO: Unfold this statements.
% There are negative effects from games \cite{AggressionFromGames}, \cite{ObesityFromGames}.
% There are positive effects from games \cite{BenefitsFromGames}, \cite{LearningFromGames}.

Because modern games are very demanding to computing resources, they constantly provoke advances in hardware and software solutions (CITE).

First-person shooter games is a genre of fast-paced video games focused on combat actions.
These games usually involve bots that navigate the environment, shoot at enemies and collect useful items.
This genre of games attracted considerable attention from game AI research community \cite{MLinFPS}.

Decision making in multiplayer first-person shooters introduces many challenges for computer algorithms.
The algorithm should possess a variety of skills in order to perform successfully in the game, 
for example ability to predict enemy's actions, long-term planning, cooperation with other bots.

% ----- PREVIOUS METHODS -----

The approaches for developing game AI have considerably evolved over time starting with simple finite-state automata (CITE), and the moving to more complex but still handwritten rule-based systems \cite{RuleBased} and fuzzy logic technique \cite{FuzzyLogic}. An overview of this and other approaches is given in \cite{AITechniques}.
Over time, more general approaches utilizing learning appeared,
including hierarchical controllers based on neural networks \cite{HierarchicalController} and usage of genetic algorithms to perform parameter tuning in predefined algorithm \cite{GeneticTuning}.
More recently, reinforcement learning was successfully used to develop game AI \cite{RLGallagher}.
Hybrid approach with human in the loop is described in \cite{InteractiveGallagher}.

Another branch of research addressed the problem of developing `believable' human-like bots \cite{BelievableBots} by using imitation learning techniques \cite{UT2}. This is important because people enjoy more playing with human-like bots (CITE).

% ----- DEEP REINFORCEMENT LEARNING -----

The general problem with all approaches is that they assume a lot of domain knowledge and are difficult to transfer between games. An ideal system would have a minimum amount of game-specific code and will be able to adapt to different conditions and learn different skills given enough time and data.
One promising way for achieving this is combining ideas from deep learning and reinforcement learning.
The area of deep learning has been rapidly developing in the latest years providing many useful
instruments for solving computer vision tasks of image recognition, most notably on ImageNet dataset \cite{AlexNet}. 
One of the main component of the state-of-the-art reinforcement learning agents are deep neural networks that are used as function approximators for state-value function or policy function.
This allowed RL agents to be trained on raw pixels in end-to-end fashion greatly increasing training speed by taking the human and custom preprocessing steps out of the loop.

This approach has proven to be highly successful in gaming domain by outperforming human on visually rich Atari games \cite{Atari} and beating world champion in classic board game of Go \cite{AlphaGo}.
There are also promising applications of Deep RL in high-dimensional moving systems such as humanoid robotics \cite{Robotics}.

Application of recurrent neural networks\cite{RLRNN} and memory units (e.g. LSTMs \cite{TextGamesLSTM}) helped to tackle problems that require long-term planning.
Finally, transfer learning (CITE) and curriculum learning \cite{Curriculum} techniques allowed to reuse the same architecture to handle different tasks and also greatly stabilized training.

The benefit of this approach is that it's purely data-driven and assumes bare minimum about the environment itself. On the other hand, it requires a huge amount of data and computing resources.
Fortunately, it's very easy to obtain data in video games comparing to other domains \cite{TrainInGames}. The hardware advances also were profound in the latest years, speeding up computations more then 10x using GPUs and custom accelerators for deep learning (TPUs, Nervana) (CITE).

%% Переносим этот кусок в раздел после шутера, то есть до описания методов. Это наша мотивация и она должна идти до описания основных результатов в области

The games are a perfect testbed for evaluating this algorithms for several reasons:
\begin{itemize}
    \item Ease of getting training data. For all computer games there is already available simulated environment that allows to generate an infinite amount of training data and also to evaluate the policy on-line.
    \item Low cost of error. It's safe to experiment in simulated environment as opposed to training a robot in real world where it's actions can have external consequences. 
    \item Scalability. It's easy to speedup training and evaluation by horizontally scaling it across several parallel instances of the same environment \cite{Gorila}.
    \item Auxiliary information for training. Using game engines give the ability to extract additional information about environment that can be used to accelerate training \cite{UNREAL}.
\end{itemize}
%% Label 1

There are several open-source testbeds that became de-facto standards for evaluating RL agents.
This includes simple environments such as Atari 2600 simulator \cite{bellemare13arcade} and tasks in OpenAI Gym \cite{OpenAIGym}.
And more complicated 3d environments such as DeepMind Lab \cite{DeepMindLab} and VizDoom \cite{Kempka2016ViZDoom}.

%%
% The rest of the article is organized as follows. The Section 1 presents core definitions for RL and DRL algorithms.

% The Section 2 describes the experiment made in Pong game on synthetic environment and technical aspects of implementation.

% The Section 3 describes the structure of BOT representation in Unreal Engine 4 and the model of a game, in which we will test our agent. 

